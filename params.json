{
  "name": "Practical machine learning assignment",
  "tagline": "The folder contains assignment for pml on Coursera",
  "body": "## Practical Machine Learning: Prediction Assignment Writeup\r\n\r\nThe wide application of smart devices such as Jawbone Up, Nike FuelBand and Fitbit makes it possible to \r\na large amount of data about personal activity relatively inexpensively. In this project, people were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The goal will \r\nbe to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants, to quantify how well they do it. \r\n\r\n* Data used for the assignment is kindly provided by http://groupware.les.inf.puc-rio.br/har\r\n* data for training https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\r\n* data for testing https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\r\n\r\n### Data Cleaning\r\n* Load caret and ggplot2 package for training and ploting. \r\n```R\r\nlibrary(caret)\r\nlibrary(ggplot2)\r\n```\r\n* Data loading\r\n```R\r\n## ============ Read training and testing data ==========================\r\ntraining_raw <- read.csv(\"pml-training.csv\")\r\ntesting_raw <- read.csv(\"pml-testing.csv\")\r\n```\r\n* There are too many predictors, so removing columns with space, NA. Also remove the first 4 irrelevant columns.\r\n```\r\ntraining_data <- training_raw[,colSums(is.na(training_raw)) == 0]\r\ntraining_data <- training_data[,sapply(training_data,is.numeric)] \r\ntraining_data <- training_data[,-c(1:4)]\r\n```\r\n*  First convert all elements to numerical value then conduct the correlation analysis. Highly correlated columns should be dumped keep less predictors for the prediction in the next part. \r\n```R\r\n# Dump highly correlated variables\r\nCorMat <- cor(training_data)\r\nremove <- findCorrelation(CorMat, cutoff = 0.9)\r\ntraining_data <- training_data[,-remove]\r\ntraining_data[[\"classe\"]] <- training_raw$classe\r\n```\r\n\r\n### Data Partition and prediction trials\r\n* Partition the data into training set (70%) and testing set (30%)\r\n```R\r\n# partition for training and testing set\r\nset.seed(201604)\r\ninTrain <- createDataPartition(y = training_data$classe, p =  0.6, list = FALSE)\r\ntraining_set <- training_data[inTrain,] \r\ntesting_set <- training_data[-inTrain,]\r\n```\r\n* Training: multiple methods in caret packages are used including: rpart(decision tree), gbm(boosting with trees), treebag (bagging), rf(random forest)\r\n```R\r\nprint(\"=============decision tree=========================\")\r\nmod_rpart <- train(classe ~., method = 'rpart',data = training_set)\r\npre <- predict(mod_rpart, newdata = testing_set)\r\nImp <- varImp(mod_rpart, scale = FALSE)\r\nggplot(Imp, top = 20)\r\nperformance <- confusionMatrix(pre,testing_set$classe)\r\nprint(performance)\r\nprint(performance$overall[1])\r\nprint(\"=============boosting with trees ===================\")\r\nmod_gbm <- train(classe ~., method = 'gbm',data = training_set)\r\npre <- predict(mod_gbm, newdata = testing_set)\r\nImp <- varImp(mod_gbm, scale = FALSE)\r\nggplot(Imp,top = 20)\r\nperformance <- confusionMatrix(pre,testing_set$classe)\r\nprint(performance)\r\nprint(performance$overall[1])\r\nprint(\"=============bagging ==============================\")\r\nmod_treebag <- train(classe ~., method = 'treebag',data = training_set)\r\npre <- predict(mod_treebag, newdata = testing_set)\r\nImp <- varImp(mod_treebag, scale = FALSE)\r\nggplot(Imp,top = 20)\r\nperformance <- confusionMatrix(pre,testing_set$classe)\r\nprint(performance)\r\nprint(performance$overall[1])\r\nprint(\"=============random forest==========================\")\r\nfit_control <- trainControl(method = \"cv\",number = 3, allowParallel =T, verbose = T)\r\nmod_rf <- train(classe ~ ., method = 'rf', data = training_set, trControl = fit_control, verbose = T)\r\npre <- predict(mod_rf, newdata = testing_set)\r\nImp <- varImp(mod_rf, scale = FALSE)\r\nggplot(Imp,top = 20)\r\nperformance <- confusionMatrix(pre,testing_set$classe)\r\nprint(performance)\r\nprint(performance$overall[1])\r\n```\r\n##### Performance of each methods\r\n* **decision tree**: the accuracy is 0.5173 which is too low.\r\n\r\n```\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\nPrediction    A    B    C    D    E\r\n         A 2014  580  637  513  301\r\n         B   51  517   54   28  243\r\n         C  132  328  538  126  309\r\n         D   34   92  139  519  118\r\n         E    1    1    0  100  471\r\n\r\nOverall Statistics\r\n                                          \r\n               Accuracy : 0.5173          \r\n                 95% CI : (0.5062, 0.5284)\r\n    No Information Rate : 0.2845          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.3709          \r\n Mcnemar's Test P-Value : < 2.2e-16       \r\n\r\nStatistics by Class:\r\n\r\n                     Class: A Class: B Class: C Class: D Class: E\r\nSensitivity            0.9023  0.34058  0.39327  0.40358  0.32663\r\nSpecificity            0.6382  0.94058  0.86184  0.94162  0.98407\r\nPos Pred Value         0.4979  0.57895  0.37544  0.57539  0.82199\r\nNeg Pred Value         0.9426  0.85603  0.87058  0.88954  0.86649\r\nPrevalence             0.2845  0.19347  0.17436  0.16391  0.18379\r\nDetection Rate         0.2567  0.06589  0.06857  0.06615  0.06003\r\nDetection Prevalence   0.5155  0.11382  0.18264  0.11496  0.07303\r\nBalanced Accuracy      0.7703  0.64058  0.62756  0.67260  0.65535\r\n```\r\n* **boosting with trees**:\r\n```\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\nPrediction    A    B    C    D    E\r\n         A 2196   51    0    1    2\r\n         B   21 1408   52    8   21\r\n         C    9   53 1294   45   10\r\n         D    3    2   21 1216   18\r\n         E    3    4    1   16 1391\r\n\r\nOverall Statistics\r\n                                          \r\n               Accuracy : 0.9565          \r\n                 95% CI : (0.9518, 0.9609)\r\n    No Information Rate : 0.2845          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.945           \r\n Mcnemar's Test P-Value : 4.692e-08       \r\n\r\nStatistics by Class:\r\n\r\n                     Class: A Class: B Class: C Class: D Class: E\r\nSensitivity            0.9839   0.9275   0.9459   0.9456   0.9646\r\nSpecificity            0.9904   0.9839   0.9819   0.9933   0.9963\r\nPos Pred Value         0.9760   0.9325   0.9171   0.9651   0.9830\r\nNeg Pred Value         0.9936   0.9826   0.9885   0.9894   0.9921\r\nPrevalence             0.2845   0.1935   0.1744   0.1639   0.1838\r\nDetection Rate         0.2799   0.1795   0.1649   0.1550   0.1773\r\nDetection Prevalence   0.2868   0.1925   0.1798   0.1606   0.1803\r\nBalanced Accuracy      0.9871   0.9557   0.9639   0.9694   0.9804\r\n```\r\n* **bagging** \r\n```\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\nPrediction    A    B    C    D    E\r\n         A 2219   18    0    0    0\r\n         B    8 1468   13    0    1\r\n         C    3   23 1345   20    5\r\n         D    1    7   10 1263    2\r\n         E    1    2    0    3 1434\r\n\r\nOverall Statistics\r\n                                          \r\n               Accuracy : 0.9851          \r\n                 95% CI : (0.9822, 0.9877)\r\n    No Information Rate : 0.2845          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.9811          \r\n Mcnemar's Test P-Value : 0.002177        \r\n\r\nStatistics by Class:\r\n\r\n                     Class: A Class: B Class: C Class: D Class: E\r\nSensitivity            0.9942   0.9671   0.9832   0.9821   0.9945\r\nSpecificity            0.9968   0.9965   0.9921   0.9970   0.9991\r\nPos Pred Value         0.9920   0.9852   0.9635   0.9844   0.9958\r\nNeg Pred Value         0.9977   0.9921   0.9964   0.9965   0.9988\r\nPrevalence             0.2845   0.1935   0.1744   0.1639   0.1838\r\nDetection Rate         0.2828   0.1871   0.1714   0.1610   0.1828\r\nDetection Prevalence   0.2851   0.1899   0.1779   0.1635   0.1835\r\nBalanced Accuracy      0.9955   0.9818   0.9877   0.9895   0.9968\r\n```\r\n\r\n\r\n\r\n* **random forest**\r\n```\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\nPrediction    A    B    C    D    E\r\n         A 2229    1    0    0    0\r\n         B    2 1515    4    0    0\r\n         C    0    2 1360    5    2\r\n         D    0    0    4 1281    1\r\n         E    1    0    0    0 1439\r\n\r\nOverall Statistics\r\n                                          \r\n               Accuracy : 0.9972          \r\n                 95% CI : (0.9958, 0.9982)\r\n    No Information Rate : 0.2845          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.9965          \r\n Mcnemar's Test P-Value : NA              \r\n\r\nStatistics by Class:\r\n\r\n                     Class: A Class: B Class: C Class: D Class: E\r\nSensitivity            0.9987   0.9980   0.9942   0.9961   0.9979\r\nSpecificity            0.9998   0.9991   0.9986   0.9992   0.9998\r\nPos Pred Value         0.9996   0.9961   0.9934   0.9961   0.9993\r\nNeg Pred Value         0.9995   0.9995   0.9988   0.9992   0.9995\r\nPrevalence             0.2845   0.1935   0.1744   0.1639   0.1838\r\nDetection Rate         0.2841   0.1931   0.1733   0.1633   0.1834\r\nDetection Prevalence   0.2842   0.1939   0.1745   0.1639   0.1835\r\nBalanced Accuracy      0.9992   0.9985   0.9964   0.9977   0.9989\r\n```\r\n#### Choosing prediction model\r\nThe accuracy of prediction made by random forest model (0.9972) is the highest of all models.\r\nBelow is the importance of variable of the random forest\r\n![](https://github.com/aureole-420/practical_machine_learning_assignment/blob/master/rf.png)\r\n\r\n\r\n## Out of sample accuracy\r\nThe analysis above shows the random forest with the best performance in prediction, so it will be used for prediction for testing set.\r\n```\r\noospre <- predict(mod_rf, newdata = testing_raw)\r\nprint(\"random forest\")\r\nprint(oospre)\r\n```\r\nThe results is displayed below which one can check in the following quiz to be all correct.\r\n```\r\n[1] \"random forest\"\r\n> print(oospre)\r\n [1] B A B A A E D B A A B C B A E E A B B B\r\nLevels: A B C D E\r\n```\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}